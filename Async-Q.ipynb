{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Reinforcement Learning with Q-Learning\n",
    "\n",
    "1. Install requirements with `pip install -r requirements.txt`\n",
    "2. Run this notebook.\n",
    "3. Monitor learning progress with `tensorboard --logdir=worker_0:'./train_0',worker_1:'./train_1',worker_2:'./train_2',worker_3:'./train_3',...worker_n:'./train_n'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "import multiprocessing\n",
    "import threading\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "from helper import *\n",
    "from gridworld_rewards import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Helper Functions\n",
    "`ExperienceBuffer` is used to store a history of experiences that can be randomly drawn from when training the network.\n",
    "\n",
    "Additional helper functions are located in `helper.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class ExperienceBuffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(list(self.buffer)) + len(list(experience)) >= self.buffer_size:\n",
    "            self.buffer[0:(len(list(experience))+len(list(self.buffer)))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self,size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer,size)),[size,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Q-Network\n",
    "This class contain the definition of the neural network in Tensorflow, including the tensorflow ops that will be required for updating the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class QNetwork():\n",
    "    def __init__(self,a_size,scope,trainer):\n",
    "        with tf.variable_scope(scope):\n",
    "            #Input and visual encoding layers\n",
    "            self.observation = tf.placeholder(shape=[None,5,5,3],dtype=tf.float32)\n",
    "            self.hidden_o = slim.fully_connected(slim.flatten(self.observation),128,activation_fn=tf.nn.elu)\n",
    "            hidden_output = slim.fully_connected(self.hidden_o,256,activation_fn=tf.nn.elu)\n",
    "\n",
    "            #We calculate separate value and advantage streams, then combine then later\n",
    "            #This technique is described in https://arxiv.org/pdf/1511.06581.pdf\n",
    "            self.expectation = slim.fully_connected(hidden_output,a_size,\n",
    "                activation_fn=None,\n",
    "                biases_initializer=None)\n",
    "            self.advantages = slim.fully_connected(hidden_output,a_size,\n",
    "                activation_fn=None,\n",
    "                biases_initializer=None)\n",
    "            self.advantages = self.advantages - tf.reduce_mean(self.advantages,reduction_indices=1,keep_dims=True)\n",
    "            self.prediction = self.expectation + self.advantages\n",
    "            \n",
    "            # We use a softmax with temperate to pick actions. This is instead of e-greedy.\n",
    "            # For more info on action-selection strategies, see: \n",
    "            # goo.gl/oyL5Vx\n",
    "            \n",
    "            self.temperature = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "            self.boltzmann = tf.nn.softmax(self.prediction/self.temperature)\n",
    "            \n",
    "            self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "            self.actions_onehot = tf.one_hot(self.actions,a_size,dtype=tf.float32)\n",
    "            \n",
    "            self.pred_action = tf.reduce_sum(self.prediction * self.actions_onehot, [1])\n",
    "            \n",
    "            #Only the global network need ops for loss functions and gradient updating.\n",
    "            if scope == 'global':\n",
    "                self.target = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "                \n",
    "                #Loss function\n",
    "                self.loss = tf.reduce_sum(tf.squared_difference(self.pred_action,self.target))\n",
    "                \n",
    "                #Entropy tells us how diverse our action probabilities are\n",
    "                self.entropy = -tf.reduce_sum(self.boltzmann * tf.log(self.boltzmann + 1e-7))\n",
    "\n",
    "                #Get gradients from network\n",
    "                global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'global')\n",
    "                self.gradients = tf.gradients(self.loss,global_vars)\n",
    "                self.var_norms = tf.global_norm(global_vars)\n",
    "                grads,self.grad_norms = tf.clip_by_global_norm(self.gradients,9999.0)\n",
    "                self.apply_grads = trainer.apply_gradients(zip(grads,global_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Worker Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "With asynchronous learning we have multiple 'worker agents,' each of which interacts with their own environment and collects experiences using its own local network. At the end of an episode those experiences are sent to the experience buffer. A random batch of experiences are then drawn from the buffer and the 'global network'  processes them and updates itself with backpropogation. The new global network is then copied over to the worker agent, and the process repeats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Worker():\n",
    "    def __init__(self,game,name,a_size,trainer,model_path,global_episodes,exp_buff,master,gif_path):\n",
    "        self.name = \"worker_\" + str(name)\n",
    "        self.number = name        \n",
    "        self.global_net = master\n",
    "        self.exp_buff = exp_buff\n",
    "        self.model_path = model_path\n",
    "        self.gif_path = gif_path\n",
    "        self.trainer = trainer\n",
    "        self.global_episodes = global_episodes\n",
    "        self.increment = self.global_episodes.assign_add(1)\n",
    "        self.episode_deliveries = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_mean_values = []\n",
    "        self.summary_writer = tf.summary.FileWriter(\"train_\"+str(self.number))\n",
    "\n",
    "        #Create the local copy of the network and the tensorflow op to copy global paramters to local network\n",
    "        self.local_Q = QNetwork(a_size,self.name,trainer)\n",
    "        self.update_local_ops = update_target_graph('global',self.name)        \n",
    "        self.env = game\n",
    "        \n",
    "    def train(self,rollout,sess):\n",
    "        rollout = np.array(rollout)\n",
    "        self.exp_buff.add(zip(rollout))\n",
    "        \n",
    "        if len(self.exp_buff.buffer) > 128:\n",
    "            exp_batch = self.exp_buff.sample(128)\n",
    "            feed_dict = {self.global_net.observation:np.stack(exp_batch[:,0],axis=0),\n",
    "                self.global_net.actions:exp_batch[:,1],\n",
    "                self.global_net.target:exp_batch[:,2]}\n",
    "            loss,g_n,v_n,_ = sess.run([self.global_net.loss,\n",
    "                self.global_net.grad_norms,\n",
    "                self.global_net.var_norms,\n",
    "                self.global_net.apply_grads],feed_dict=feed_dict)\n",
    "            return loss / len(rollout), g_n,v_n\n",
    "        else:\n",
    "            return 0,0,0\n",
    "        \n",
    "    def work(self,sess,coord,saver,train):\n",
    "        episode_count = sess.run(self.global_episodes)\n",
    "        self.episode_count = episode_count\n",
    "        total_steps = 0\n",
    "        print(\"Starting worker \" + str(self.number))\n",
    "        with sess.as_default(), sess.graph.as_default():                 \n",
    "            while not coord.should_stop():\n",
    "                sess.run(self.update_local_ops)\n",
    "                episode_buffer = []\n",
    "                episode_frames = []\n",
    "                episode_rewards = 0\n",
    "                d = False\n",
    "                t = 0\n",
    "                temp = 0.25\n",
    "                \n",
    "                s,o_big,m,g,h = self.env.reset()\n",
    "\n",
    "                while d == False:\n",
    "                    a_dist = sess.run(self.local_Q.boltzmann, \n",
    "                        feed_dict={self.local_Q.observation:[s],self.local_Q.temperature:[temp]})\n",
    "                    a = np.random.choice(a_dist[0],p=a_dist[0])\n",
    "                    a = np.argmax(a_dist[0] == a)\n",
    "                    \n",
    "                    s1,s1_big,r,g1,h1,d = self.env.step(a)      \n",
    "                    \n",
    "                    #The Q-learning update rule\n",
    "                    # We use this to generate target values to update our Q-network toward\n",
    "                    if d == True:\n",
    "                        y = r\n",
    "                    else:\n",
    "                        self.qnext = sess.run(self.local_Q.prediction,feed_dict={self.local_Q.observation:[s1]})\n",
    "                        y = r + 0.95*np.max(self.qnext)\n",
    "                    \n",
    "                    episode_rewards += r\n",
    "                    episode_buffer.append([s,a,y])\n",
    "                    if self.name == 'worker_0' and episode_count % 150 == 0:\n",
    "                        episode_frames.append(set_image_gridworld_reward(s1_big,episode_rewards,t+1,g1,h1))\n",
    "                    total_steps += 1\n",
    "                    s = s1\n",
    "                    g = g1\n",
    "                    h = h1\n",
    "                    t += 1\n",
    "                    \n",
    "                    if t > 100:\n",
    "                        d = True\n",
    "                                            \n",
    "                self.episode_deliveries.append(episode_rewards)\n",
    "                self.episode_lengths.append(t)\n",
    "                \n",
    "                # Update the network using the experience buffer at the end of the episode.\n",
    "                if train == True:\n",
    "                    loss,g_n,v_n = self.train(episode_buffer,sess)\n",
    "            \n",
    "                    \n",
    "                # Periodically save gifs of episodes, model parameters, and summary statistics.\n",
    "                if episode_count % 50 == 0 and episode_count != 0:\n",
    "                    if episode_count % 2000 == 0 and self.name == 'worker_0' and train == True:\n",
    "                        saver.save(sess,self.model_path+'/model-'+str(episode_count)+'.cptk')\n",
    "                        print(\"Saved Model\")\n",
    "\n",
    "                    if self.name == 'worker_0' and episode_count % 150 == 0:\n",
    "                        time_per_step = 0.25\n",
    "                        self.images = np.array(episode_frames)\n",
    "                        imageio.mimsave(self.gif_path+'/image'+str(episode_count)+'.gif',self.images, duration=time_per_step)\n",
    "                        \n",
    "                    mean_deliveries = np.mean(self.episode_deliveries[-50:])\n",
    "                    mean_length = np.mean(self.episode_lengths[-50:])\n",
    "                    mean_value = np.mean(self.episode_mean_values[-50:])\n",
    "                    summary = tf.Summary()\n",
    "                    summary.value.add(tag='Performance/Deliveries', simple_value=float(mean_deliveries))\n",
    "                    if train == True:\n",
    "                        summary.value.add(tag='Losses/Loss', simple_value=float(loss))\n",
    "                        summary.value.add(tag='Losses/Grad Norm', simple_value=float(g_n))\n",
    "                    self.summary_writer.add_summary(summary, episode_count)\n",
    "\n",
    "                    self.summary_writer.flush()\n",
    "                if self.name == 'worker_0':\n",
    "                    sess.run(self.increment)\n",
    "                episode_count += 1\n",
    "                self.episode_count = episode_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training the network\n",
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "a_size = 4 # Number of available actions\n",
    "load_model = False #Whether to load the model or start training from scratch\n",
    "train = True #Whether to train the model or simply use it for solving the task\n",
    "model_path = './model_Q' #The location to save the model to\n",
    "gif_path = './frames_Q' #The location to save gifs of the agent-environemnt interaction to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The below code establishes the global tensorflow network, as well as creating and starting each of the workers with their own individual networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "exp_buff = ExperienceBuffer()\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "    \n",
    "if not os.path.exists(gif_path):\n",
    "    os.makedirs(gif_path)\n",
    "\n",
    "trainer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "master_network = QNetwork(a_size,'global',trainer) # Generate global network\n",
    "with tf.device(\"/cpu:0\"): \n",
    "    global_episodes = tf.Variable(0,dtype=tf.int32,name='global_episodes',trainable=False)\n",
    "    num_workers = multiprocessing.cpu_count() # Set workers ot number of available CPU threads\n",
    "    workers = []\n",
    "    # Create worker classes\n",
    "    for i in range(num_workers):\n",
    "        workers.append(\n",
    "        Worker(gameEnv(partial=False,size=5),\n",
    "        i,a_size,trainer,model_path,global_episodes,\n",
    "        exp_buff,master_network,gif_path))\n",
    "    saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    # Start each of the workers on a separate thread\n",
    "    worker_threads = []\n",
    "    for worker in workers:\n",
    "        worker_work = lambda: worker.work(sess,coord,saver,train)\n",
    "        thread = threading.Thread(target=(worker_work))\n",
    "        thread.start()\n",
    "        time.sleep(0.5)\n",
    "        worker_threads.append(thread)\n",
    "    coord.join(worker_threads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
